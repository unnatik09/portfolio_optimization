{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e723d6-9886-462c-9f32-9f6d719d6301",
   "metadata": {},
   "source": [
    "## 📘 Project Summary: Predictive Portfolio Optimization with Sentiment Overlay\n",
    "\n",
    "This notebook presents an end-to-end pipeline for optimizing a stock portfolio using predicted returns and sentiment signals.\n",
    "\n",
    "### 🔧 Feature Engineering\n",
    "- Extracted **technical indicators** like RSI, MACD, ADX, volatility, Bollinger Band Width, etc.\n",
    "- Added **market regime signals** based on NIFTY trend classification (bull, bear, neutral).\n",
    "- Created a **market snapshot** using 5 years of OHLCV data for top 30 NIFTY stocks.\n",
    "\n",
    "### 📈 Return Prediction Model\n",
    "- Used an **ensemble** of **LightGBM (70%)** and **Ridge Regression (30%)** for multi-output return forecasting.\n",
    "- Ensemble achieved **balanced Train/Test R² scores**, avoiding overfitting.\n",
    "- Tried **Optuna**, **MLP**, and **PCA**, but they were either unstable or overfit the data.\n",
    "\n",
    "### 💹 Portfolio Optimization\n",
    "- Used **PyPortfolioOpt's Efficient Frontier** to maximize the **Sharpe Ratio** based on predicted returns.\n",
    "- Simulated the portfolio’s **cumulative return**, **drawdowns**, and compared it with:\n",
    "  - 📈 ML-optimized strategy\n",
    "  - ⚖️ Equal-weighted strategy\n",
    "  - 📉 NIFTY 50 benchmark\n",
    "\n",
    "### 📰 Sentiment Integration (FinBERT)\n",
    "- Extracted **news headlines** using **NewsAPI** and scored them via **FinBERT (PyTorch)**.\n",
    "- Applied sentiment overlay to returns using:\n",
    "\n",
    "  $$\n",
    "  \\mu_{\\text{sent}} = \\mu_{\\text{pred}} \\times (1 + \\alpha \\times \\text{sentiment\\_score})\n",
    "  $$\n",
    "\n",
    "- Re-optimized portfolio and found **improved Sharpe Ratio over last 30 days**.\n",
    "\n",
    "### 💰 Final Allocation\n",
    "- Generated final weights and allocation for ₹1,00,000 based on today’s sentiment.\n",
    "- Saved daily CSVs for deployment and comparison\n",
    "\n",
    "> ✅ **Result**: A robust, explainable, and sentiment-aware portfolio strategy that outperformed the equal-weighted and index benchmarks on recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1a382-e07a-4ccd-ba41-b36208573e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic libraries and some for feature engineering\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import ta\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#for multioutput model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "#for visualisations\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "\n",
    "#for sentiment analysis and porfolio optimization\n",
    "from newsapi import NewsApiClient\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from pypfopt import EfficientFrontier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7404bcb-3538-4f09-b5f1-954f36d80981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute technical and signal features\n",
    "\n",
    "def compute_features(df_price, df_volume, nifty_close):\n",
    "\n",
    "    df = pd.DataFrame(index=df_price.index)\n",
    "\n",
    "    # --- Returns ---\n",
    "    df[\"log_ret_1d\"] = np.log(df_price / df_price.shift(1))\n",
    "    df[\"log_ret_5d\"] = np.log(df_price / df_price.shift(5))\n",
    "\n",
    "    # --- Volatility ---\n",
    "    df[\"vol_5d\"] = df[\"log_ret_1d\"].rolling(5).std()\n",
    "    df[\"vol_20d\"] = df[\"log_ret_1d\"].rolling(20).std()\n",
    "\n",
    "    # --- RSI ---\n",
    "    df[\"rsi_14\"] = ta.momentum.RSIIndicator(close=df_price, window=14).rsi()\n",
    "\n",
    "    # --- Moving Averages ---\n",
    "    df[\"ma_5d\"] = df_price.rolling(5).mean()\n",
    "    df[\"ma_20d\"] = df_price.rolling(20).mean()\n",
    "\n",
    "    # --- MACD ---\n",
    "    macd = ta.trend.MACD(close=df_price)\n",
    "    df[\"macd_diff\"] = macd.macd_diff()\n",
    "\n",
    "    # --- Bollinger Band Width ---\n",
    "    bb = ta.volatility.BollingerBands(close=df_price, window=20)\n",
    "    df[\"bollinger_width\"] = bb.bollinger_wband()\n",
    "\n",
    "    # --- CCI ---\n",
    "    df[\"cci\"] = ta.trend.CCIIndicator(high=df_price, low=df_price, close=df_price, window=20).cci()\n",
    "\n",
    "    # --- ADX ---\n",
    "    adx = ta.trend.ADXIndicator(high=df_price, low=df_price, close=df_price, window=14)\n",
    "    df[\"adx\"] = adx.adx()\n",
    "\n",
    "    # --- Volume Change ---\n",
    "    df[\"vol_chg\"] = df_volume.pct_change()\n",
    "\n",
    "    # --- Price Ratios ---\n",
    "    df[\"high_low_ratio\"] = (df_price.rolling(1).max() - df_price.rolling(1).min()) / df_price.rolling(1).min()\n",
    "    df[\"close_open_ratio\"] = (df_price - df_price.shift(1)) / df_price.shift(1)\n",
    "\n",
    "    # --- Market Regime ---\n",
    "    nifty = pd.DataFrame(index=nifty_close.index)\n",
    "    nifty[\"daily_return\"] = nifty_close.pct_change()\n",
    "    nifty[\"rolling_mean_20d\"] = nifty[\"daily_return\"].rolling(20).mean()\n",
    "\n",
    "    def classify_regime(x):\n",
    "        if x > 0.001:\n",
    "            return \"bull\"\n",
    "        elif x < -0.001:\n",
    "            return \"bear\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    nifty[\"regime\"] = nifty[\"rolling_mean_20d\"].apply(classify_regime)\n",
    "    df[\"market_regime\"] = df.index.map(nifty[\"regime\"])\n",
    "    df = pd.get_dummies(df, columns=[\"market_regime\"])  # One-hot encode\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(method=\"ffill\", inplace=True)\n",
    "    df.fillna(method=\"bfill\", inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a828b-712f-4895-b43a-08f6879ee034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Working with top 30 companies in NIFTY 50\n",
    "nifty_30 = [\n",
    "    'RELIANCE.NS', 'TCS.NS', 'INFY.NS', 'HDFCBANK.NS', 'ICICIBANK.NS',\n",
    "    'LT.NS', 'SBIN.NS', 'KOTAKBANK.NS', 'AXISBANK.NS', 'HCLTECH.NS',\n",
    "    'ITC.NS', 'ASIANPAINT.NS', 'ULTRACEMCO.NS', 'NESTLEIND.NS', 'HINDUNILVR.NS',\n",
    "    'BAJFINANCE.NS', 'ADANIENT.NS', 'POWERGRID.NS', 'COALINDIA.NS', 'BHARTIARTL.NS',\n",
    "    'NTPC.NS', 'TITAN.NS', 'ONGC.NS', 'SUNPHARMA.NS', 'TECHM.NS',\n",
    "    'DIVISLAB.NS', 'WIPRO.NS', 'MARUTI.NS', 'TATAMOTORS.NS', 'BPCL.NS'\n",
    "]\n",
    "\n",
    "tickers = nifty_30\n",
    "\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "nifty_close = yf.download(\"^NSEI\", start=start_str, end=end_str)[\"Close\"]\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start=start_str, end=end_str, auto_adjust=True)\n",
    "\n",
    "    if df.empty or 'Close' not in df or 'Volume' not in df:\n",
    "        print(f\"Skipping {ticker} due to missing data.\")\n",
    "        continue\n",
    "\n",
    "    price = df['Close'].squeeze()\n",
    "    volume = df['Volume'].squeeze()\n",
    "\n",
    "    features = compute_features(price, volume, nifty_close)\n",
    "    features = features.add_prefix(f\"{ticker}_\")\n",
    "    all_features.append(features)\n",
    "\n",
    "market_snapshot = pd.concat(all_features, axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcdcef-d498-4ab5-9113-2e20f1841785",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = yf.download(tickers, start=start_str, end=end_str, auto_adjust=True)[\"Close\"]\n",
    "\n",
    "future_returns = pd.DataFrame(index=prices.index)\n",
    "\n",
    "for ticker in prices.columns:\n",
    "    try:\n",
    "        future_price = prices[ticker].shift(-5)\n",
    "        future_returns[ticker] = ((future_price - prices[ticker]) / prices[ticker]) \n",
    "    except Exception as e:\n",
    "        print(f\"Error in {ticker}: {e}\")\n",
    "\n",
    "nf = market_snapshot.join(future_returns, how='inner')\n",
    "nf = nf.dropna()\n",
    "nf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b59e0-8b3c-4dda-84a5-a86c6e38b897",
   "metadata": {},
   "source": [
    "### 📊 Line Plots of Key Technical Indicators<br>\n",
    "This interactive figure presents time-series line plots of several technical indicators computed for select stocks in the portfolio:<br>\n",
    "\t•\t**RSI (14)**: Measures momentum; values above 70 may indicate overbought conditions, while below 30 may signal oversold.<br>\n",
    "\t•\t**MACD Difference**: Reflects momentum by capturing the divergence between fast and slow EMAs; a rising MACD may indicate bullish momentum.<br>\n",
    "\t•\t**ADX (14)**: Measures trend strength; values above 25 suggest strong trends.<br>\n",
    "\t•\t**Bollinger Band Width**: Captures volatility by measuring the distance between upper and lower bands.<br>\n",
    "\t•\t**5-Day Rolling Volatility**: Represents short-term price fluctuation magnitude.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac307fc-72a0-43b2-9396-762269149c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tickers = [\"RELIANCE.NS\", \"TCS.NS\", \"HDFCBANK.NS\"]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=5, subplot_titles=[\n",
    "    \"RSI (14)\", \"MACD Diff\", \"ADX\", \"Bollinger Width\", \"Volatility (5d)\"\n",
    "], horizontal_spacing=0.05)\n",
    "\n",
    "for ticker in sample_tickers:\n",
    "    fig.add_trace(go.Scatter(y=nf[f\"{ticker}_rsi_14\"], mode=\"lines\", name=ticker), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(y=nf[f\"{ticker}_macd_diff\"], mode=\"lines\", name=ticker), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(y=nf[f\"{ticker}_adx\"], mode=\"lines\", name=ticker), row=1, col=3)\n",
    "    fig.add_trace(go.Scatter(y=nf[f\"{ticker}_bollinger_width\"], mode=\"lines\", name=ticker), row=1, col=4)\n",
    "    fig.add_trace(go.Scatter(y=nf[f\"{ticker}_vol_5d\"], mode=\"lines\", name=ticker), row=1, col=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"📈 Line Plot of Technical Indicators (Scrollable)\",\n",
    "    width=2500,\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Render as scrollable HTML\n",
    "html = f\"\"\"\n",
    "<div style=\"overflow-x:auto; width:100%\">\n",
    "  {pio.to_html(fig, include_plotlyjs='cdn', full_html=False)}\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c44b4-8a5f-4dda-bf00-29c42bc11667",
   "metadata": {},
   "source": [
    "### 📊 Market Regime Heatmap (Interactive - Selected Stocks)\n",
    "\n",
    "This interactive heatmap shows **market regime classifications** (bull, bear, neutral) across time for 3 representative stocks. Regimes were derived using NIFTY 50’s 20-day rolling return and one-hot encoded.\n",
    "\n",
    "- Dark regions represent active regimes (Bull/Bear/Neutral)\n",
    "- Helps visually trace macroeconomic phases affecting each stock\n",
    "\n",
    "Reducing the number of stocks improves clarity without losing interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129351c-b55d-43a8-a72e-6f6ed30117be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tickers = [\"RELIANCE.NS\", \"HDFCBANK.NS\", \"SUNPHARMA.NS\"]\n",
    "\n",
    "# Get regime-related columns for those tickers\n",
    "regime_cols = [col for col in nf.columns if any(t in col for t in sample_tickers) and \"market_regime\" in col]\n",
    "heatmap_df = nf[regime_cols].copy()\n",
    "heatmap_df[\"Date\"] = nf.index\n",
    "\n",
    "# Reshape for plotting\n",
    "df_melt = heatmap_df.melt(id_vars=\"Date\", var_name=\"Ticker_Regime\", value_name=\"Value\")\n",
    "\n",
    "# Pivot for heatmap: fill missing values with 0 before converting to int\n",
    "pivot_df = df_melt.pivot(index=\"Ticker_Regime\", columns=\"Date\", values=\"Value\").fillna(0).astype(int)\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.imshow(\n",
    "    pivot_df,\n",
    "    color_continuous_scale=[[0, \"white\"], [1, \"navy\"]],\n",
    "    labels={\"color\": \"Active\"},\n",
    "    title=\"📊 Market Regime Heatmap (Selected Stocks)\"\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, margin=dict(l=50, r=50, t=60, b=50))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b95ac6-0a2b-48d5-aa16-6c67b71d8326",
   "metadata": {},
   "source": [
    "### 📦 Boxplot of 20-Day Rolling Volatility (across stocks)\n",
    "\n",
    "This boxplot summarizes the distribution of 20-day rolling volatility for each stock.\n",
    "It helps highlight:\n",
    "\n",
    "- Stocks with consistently high or low volatility\n",
    "- Outliers and variability in returns\n",
    "- Relative risk across different assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605e35e-3441-43ea-8bff-097b5ab7b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all volatility columns\n",
    "vol_cols = [col for col in nf.columns if col.endswith(\"_vol_20d\")]\n",
    "vol_data = nf[vol_cols].copy()\n",
    "\n",
    "# Melt for boxplot\n",
    "vol_melted = vol_data.melt(var_name=\"Stock\", value_name=\"20D Volatility\")\n",
    "\n",
    "fig = px.box(\n",
    "    vol_melted,\n",
    "    x=\"Stock\",\n",
    "    y=\"20D Volatility\",\n",
    "    title=\"📦 20-Day Rolling Volatility Across Stocks\",\n",
    "    color_discrete_sequence=[\"mediumpurple\"]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=45,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7b989-4f7a-438c-9e09-724cf414a552",
   "metadata": {},
   "source": [
    "### 🔥 Correlation of a Single Feature Across Stocks\n",
    "\n",
    "This heatmap shows the correlation between stocks based on a single feature (e.g., RSI or MACD).\n",
    "It helps identify:\n",
    "\n",
    "- Clusters of stocks that move similarly in that indicator\n",
    "- Potential for diversification or concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ebc0ff-d1f4-422c-b749-698b736e3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# Pick a feature (e.g., RSI)\n",
    "feature = \"rsi_14\"\n",
    "feature_cols = [col for col in nf.columns if col.endswith(f\"_{feature}\")]\n",
    "data = nf[feature_cols].copy()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "labels = [col.replace(f\"_{feature}\", \"\") for col in feature_cols]\n",
    "\n",
    "# Create heatmap without annotations\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=labels,\n",
    "    y=labels,\n",
    "    colorscale='RdBu',\n",
    "    zmin=-1, zmax=1,\n",
    "    colorbar=dict(title=\"Correlation\")\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"📊 Correlation Heatmap of {feature.upper()} Across Stocks\",\n",
    "    xaxis=dict(showgrid=False),\n",
    "    yaxis=dict(showgrid=False),\n",
    "    width=700,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30082c93-180d-4e2f-a793-40371fd53947",
   "metadata": {},
   "source": [
    "### 🔥 Sector-Wise Correlation Heatmaps\n",
    "\n",
    "This interactive set of heatmaps visualizes the intra-sector correlations for different industries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5fe43-bcd6-438b-9456-cd38672bd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sector groupings\n",
    "sectors = {\n",
    "    \"Banks\": [\"HDFCBANK.NS\", \"ICICIBANK.NS\", \"KOTAKBANK.NS\", \"SBIN.NS\", \"AXISBANK.NS\"],\n",
    "    \"IT\": [\"INFY.NS\", \"TCS.NS\", \"WIPRO.NS\", \"TECHM.NS\", \"HCLTECH.NS\"],\n",
    "    \"FMCG\": [\"ITC.NS\", \"HINDUNILVR.NS\", \"NESTLEIND.NS\"],\n",
    "    \"Auto\": [\"TATAMOTORS.NS\", \"MARUTI.NS\", \"BAJFINANCE.NS\"],\n",
    "    \"Energy\": [\"RELIANCE.NS\", \"ONGC.NS\", \"NTPC.NS\", \"BPCL.NS\", \"COALINDIA.NS\"]\n",
    "}\n",
    "\n",
    "# Create individual figures for each sector\n",
    "sector_figs_html = \"\"\n",
    "for sector, tickers in sectors.items():\n",
    "    try:\n",
    "        data = nf[[f\"{ticker}_log_ret_1d\" for ticker in tickers]].copy()\n",
    "        corr = data.corr()\n",
    "\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=corr.values,\n",
    "            x=tickers,\n",
    "            y=tickers,\n",
    "            colorscale=\"RdBu\",\n",
    "            zmin=-1, zmax=1,\n",
    "            colorbar=dict(title=\"Correlation\")\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{sector} Sector Correlation\",\n",
    "            width=400,\n",
    "            height=300,\n",
    "            margin=dict(l=40, r=40, t=60, b=40)\n",
    "        )\n",
    "\n",
    "        # Export each plot as HTML snippet\n",
    "        fig_html = fig.to_html(full_html=False, include_plotlyjs=False)\n",
    "        sector_figs_html += f\"<div style='margin-right: 20px; display:inline-block;'>{fig_html}</div>\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not plot {sector}: {e}\")\n",
    "\n",
    "# Display all in a scrollable div\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"overflow-x: auto; white-space: nowrap; border:1px solid #ccc; padding:10px;\">\n",
    "    {sector_figs_html}\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a6c6b-f189-41da-b9ba-59c7aa06c033",
   "metadata": {},
   "source": [
    "### 🧪 Model Experimentation Summary\n",
    "\n",
    "**✅ Final Model: LGBM + Ridge Ensemble**<br>\n",
    "\t•\tCombines nonlinear learning of LGBM with regularization of Ridge.<br>\n",
    "\t•\tEnsemble (70% LGBM + 30% Ridge) gives higher test R², reduces overfitting, and performs consistently well across stocks.<br>\n",
    "\n",
    "**❌ PCA**<br>\n",
    "\t•\tApplied for dimensionality reduction but led to loss of interpretability and overfitting.<br>\n",
    "\t•\tPCA compressed too much signal into few components.<br>\n",
    "\n",
    "**❌ Optuna Tuning**<br>\n",
    "\t•\tImproved training R² (≈ 0.99) but caused severe overfitting.<br>\n",
    "\t•\tFinal model uses manual regularization for better generalization.<br>\n",
    "\n",
    "**❌ MLP (Neural Network)** <br>\n",
    "\t•\tPerformed poorly due to:<br>\n",
    "\t•\tTabular, non-sequential data<br>\n",
    "\t•\tLack of temporal memory<br>\n",
    "\t•\tUnderfitting and poor generalization<br>\n",
    "\n",
    "**🌀 Why Not LSTM?** <br>\n",
    "\t•\tOur data is in “snapshot” format (features for all stocks per day).<br>\n",
    "\t•\tLSTM needs sequential format per stock, which would require full pipeline redesign.<br>\n",
    "\n",
    "**🏆 Why LGBM + Ridge Wins** <br>\n",
    "\t•\tFast, robust, and generalizes well.<br>\n",
    "\t•\tHandles tabular data efficiently.<br>\n",
    "\t•\tGives best test performance with lower variance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6369bf2-4293-4445-b947-8163756f91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nf[market_snapshot.columns]\n",
    "y = nf[future_returns.columns]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LGBM Model\n",
    "lgbm = MultiOutputRegressor(LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=1.0,\n",
    "    max_depth=6,\n",
    "    num_leaves=20\n",
    "))\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred_lgbm = lgbm.predict(X_test)\n",
    "y_train_pred_lgbm = lgbm.predict(X_train)\n",
    "\n",
    "# Train Ridge Model\n",
    "ridge = MultiOutputRegressor(Ridge(alpha=1.0))\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "y_train_pred_ridge = ridge.predict(X_train)\n",
    "\n",
    "# Blend predictions (70% LGBM + 30% Ridge)\n",
    "y_pred = 0.7 * y_pred_lgbm + 0.3 * y_pred_ridge\n",
    "y_train_pred = 0.7 * y_train_pred_lgbm + 0.3 * y_train_pred_ridge\n",
    "\n",
    "# Evaluate\n",
    "r2_test = r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "r2_train = r2_score(y_train, y_train_pred, multioutput='raw_values')\n",
    "mae_scores = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "rmse_scores = np.sqrt(mean_squared_error(y_test, y_pred, multioutput='raw_values'))\n",
    "\n",
    "for i, ticker in enumerate(y.columns):\n",
    "    print(f\"{ticker} → Train R²: {r2_train[i]:.3f} | Test R²: {r2_test[i]:.3f} | MAE: {mae_scores[i]:.5f} | RMSE: {rmse_scores[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6b4c0-a549-45c9-b623-4e877ac5537e",
   "metadata": {},
   "source": [
    "**📊 Ensemble Model (0.7 LGBM + 0.3 Ridge) achieves balanced performance with most test R² scores between 0.50–0.60, indicating strong generalization and reduced overfitting across 30 stocks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d2765-60c6-4e29-97ca-1e1bcf54fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r2 = r2_score(y_train, y_train_pred, multioutput='raw_values')\n",
    "test_r2 = r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "print(\"\\n🔍 R² Scores for Ensemble (0.7 LGBM + 0.3 Ridge):\")\n",
    "for i, ticker in enumerate(y.columns):\n",
    "    print(f\"{ticker} → Train R²: {train_r2[i]:.3f} | Test R²: {test_r2[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d66f8-5465-4489-916e-8445c93c79b1",
   "metadata": {},
   "source": [
    "### 🧠 Portfolio Optimization using Predicted Returns<br>\n",
    "\n",
    "We used the ensemble model’s predicted 5-day returns to compute the expected returns (μ) for each stock and used the historical covariance matrix (Σ) of daily returns to optimize the portfolio.<br>\n",
    "\t•\tOptimization was done using PyPortfolioOpt’s Max Sharpe Ratio strategy.<br>\n",
    "\t•\tThe resulting weights are visualized using a donut-style pie chart.<br>\n",
    "\t•\tOnly stocks with non-zero weights are shown for clarity.<br>\n",
    "\t•\tFinal portfolio returns are simulated over historical data and compounded to obtain cumulative returns.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85023420-6a89-4b45-81f5-f629cf4a6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = 0.7 * lgbm.predict(X) + 0.3 * ridge.predict(X)\n",
    "\n",
    "# 1. Assign predictions back to the dataframe\n",
    "pred_columns = [f\"y_pred_{col}\" for col in y.columns]\n",
    "df_all = nf.copy()\n",
    "df_all[pred_columns] = y_pred\n",
    "\n",
    "# 2. Average predicted return per asset\n",
    "mu_pred = df_all[pred_columns].mean()\n",
    "mu_pred.index = [col.replace(\"y_pred_\", \"\") for col in mu_pred.index]\n",
    "\n",
    "# 3. Load historical daily returns\n",
    "returns = pd.read_csv(\"daily_returns.csv\", index_col=0, parse_dates=True).dropna(axis=1)\n",
    "\n",
    "# 4. Align assets between predicted returns and return matrix\n",
    "assets = list(set(mu_pred.index).intersection(set(returns.columns)))\n",
    "mu = mu_pred.loc[assets]\n",
    "S = returns[assets].cov()\n",
    "\n",
    "# 5. Optimize portfolio (maximize Sharpe ratio)\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "ef = EfficientFrontier(mu, S)\n",
    "weights = ef.max_sharpe()\n",
    "cleaned_weights = ef.clean_weights()\n",
    "weights_series = pd.Series(cleaned_weights).loc[assets]\n",
    "\n",
    "# Filter non-zero weights (optional for cleaner chart)\n",
    "nonzero_weights = weights_series[weights_series > 0]\n",
    "\n",
    "# Create pie chart\n",
    "fig = px.pie(\n",
    "    nonzero_weights,\n",
    "    names=nonzero_weights.index,\n",
    "    values=nonzero_weights.values,\n",
    "    title=\"📊 Portfolio Allocation (Optimized Weights)\",\n",
    "    hole=0.4\n",
    ")\n",
    "\n",
    "# Add labels and increase size\n",
    "fig.update_traces(textinfo='label+percent+value', textfont_size=16)\n",
    "\n",
    "# Increase figure size\n",
    "fig.update_layout(\n",
    "    width=1200,         # width in pixels\n",
    "    height=500,        # height in pixels\n",
    "    title_font_size=22\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "portfolio_returns = returns[assets] @ weights_series\n",
    "cumulative_returns = (1 + portfolio_returns).cumprod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c294df9-8ab5-49c9-ae28-00c34d34e8e8",
   "metadata": {},
   "source": [
    "### 📈 Cumulative Return — Ensemble Model\n",
    "\n",
    "This plot shows the simulated **portfolio value over time** using weights optimized on ensemble-predicted returns.\n",
    "\n",
    "- **Data**: Historical daily returns (5-year window).\n",
    "- **Model**: 0.7 × LGBM + 0.3 × Ridge ensemble.\n",
    "- **Insight**: Helps visualize portfolio growth trend and volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55908d83-5bf9-4eba-8036-f2db5bdb23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cumulative_returns.index,\n",
    "    y=cumulative_returns.values,\n",
    "    mode='lines',\n",
    "    name='Cumulative Return',\n",
    "    line=dict(color='royalblue', width=3)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"📈 Cumulative Portfolio Return (Ensemble Model)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Portfolio Value\",\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af204d7f-7343-4cbe-9b51-7b680c5c15f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Daily returns of portfolio ===\n",
    "daily_returns = portfolio_returns\n",
    "trading_days = 252\n",
    "\n",
    "# 1. Cumulative Return\n",
    "cumulative_return = cumulative_returns.iloc[-1] - 1\n",
    "\n",
    "# 2. Annualized Return\n",
    "annual_return = (1 + cumulative_return) ** (trading_days / len(daily_returns)) - 1\n",
    "\n",
    "# 3. Annualized Volatility\n",
    "annual_volatility = daily_returns.std() * np.sqrt(trading_days)\n",
    "\n",
    "# 4. Sharpe Ratio (risk-free rate assumed 0)\n",
    "sharpe_ratio = annual_return / annual_volatility if annual_volatility != 0 else np.nan\n",
    "\n",
    "# 5. Maximum Drawdown\n",
    "running_max = cumulative_returns.cummax()\n",
    "drawdown = cumulative_returns / running_max - 1\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "# === Print Metrics ===\n",
    "print(\"\\n🔎 Portfolio Performance Metrics:\")\n",
    "print(f\"Cumulative Return: {cumulative_return:.2%}\")\n",
    "print(f\"Annual Return:    {annual_return:.2%}\")\n",
    "print(f\"Annual Volatility:{annual_volatility:.2%}\")\n",
    "print(f\"Sharpe Ratio:     {sharpe_ratio:.2f}\")\n",
    "print(f\"Max Drawdown:     {max_drawdown:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663b944-2aa5-45f0-84c3-2b4949ecd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Rolling Sharpe Ratio ===\n",
    "rolling_window = 30  # 30-day rolling\n",
    "rolling_mean = daily_returns.rolling(window=rolling_window).mean()\n",
    "rolling_std = daily_returns.rolling(window=rolling_window).std()\n",
    "rolling_sharpe = (rolling_mean / rolling_std) * np.sqrt(252)\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=rolling_sharpe.index,\n",
    "    y=rolling_sharpe.values,\n",
    "    mode='lines',\n",
    "    name='Rolling Sharpe Ratio',\n",
    "    line=dict(color='seagreen', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"📉 30-Day Rolling Sharpe Ratio\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Sharpe Ratio\",\n",
    "    template=\"plotly_white\",\n",
    "    height=450,\n",
    "    width=900\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08921a1-6cc6-49d6-80f6-0e4aaf8691f7",
   "metadata": {},
   "source": [
    "📌 **Interpretation:**  \n",
    "The 30-day rolling Sharpe Ratio shows how consistently the portfolio earned risk-adjusted returns over time.  \n",
    "Peaks (above 2–3) indicate strong performance relative to volatility.  \n",
    "Dips below 0 suggest periods of underperformance or high risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead9933-8eac-4bef-bb32-13c393f2e113",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff7565a-0e5c-448a-9c0b-349bdc38c916",
   "metadata": {},
   "source": [
    "### 📈 Cumulative Return Comparison\n",
    "\n",
    "This chart compares the performance of:\n",
    "\n",
    "- **📈 ML Optimized Portfolio**: Our predicted-return-based allocation.\n",
    "- **⚖️ Equal-Weighted Portfolio**: Naively investing equally across assets.\n",
    "- **📉 NIFTY 50 Index**: Benchmark market index.\n",
    "\n",
    "The ML strategy exhibits superior cumulative return over the time horizon, validating the predictive value of our model-driven allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841d80d-4772-4bfb-95ba-f5c6066dbaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Equal Weighted Portfolio Simulation ---\n",
    "equal_weights = np.ones(len(assets)) / len(assets)\n",
    "equal_returns = returns[assets] @ pd.Series(equal_weights, index=assets)\n",
    "equal_cumulative = (1 + equal_returns).cumprod()\n",
    "\n",
    "# --- NIFTY 50 Index Benchmark ---\n",
    "nifty = yf.download(\"^NSEI\", start=returns.index[0], end=returns.index[-1])\n",
    "nifty['Date'] = pd.to_datetime(nifty.index)\n",
    "nifty.set_index('Date', inplace=True)\n",
    "nifty_daily = nifty['Close'].pct_change().fillna(0)\n",
    "nifty_cumulative = (1 + nifty_daily).cumprod()\n",
    "\n",
    "# --- Align all series by common date index ---\n",
    "common_index = cumulative_returns.index.intersection(nifty_cumulative.index).intersection(equal_cumulative.index)\n",
    "ml_cumulative = cumulative_returns.loc[common_index]\n",
    "equal_cumulative = equal_cumulative.loc[common_index]\n",
    "nifty_cumulative = nifty_cumulative.loc[common_index]\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# ML Optimized Portfolio\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ml_cumulative.index,\n",
    "    y=ml_cumulative.values,\n",
    "    mode='lines',\n",
    "    name='📈 ML Optimized Portfolio',\n",
    "    line=dict(width=3, color='mediumvioletred')\n",
    "))\n",
    "\n",
    "# Equal Weighted Portfolio\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=equal_cumulative.index,\n",
    "    y=equal_cumulative.values,\n",
    "    mode='lines',\n",
    "    name='⚖️ Equal-Weighted Portfolio',\n",
    "    line=dict(width=2, dash='dash', color='slateblue')\n",
    "))\n",
    "\n",
    "# NIFTY 50 Index\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=nifty_cumulative.index,\n",
    "    y=nifty_cumulative.values,\n",
    "    mode='lines',\n",
    "    name='📉 NIFTY 50 Index',\n",
    "    line=dict(width=2, dash='dot', color='gray')\n",
    "))\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    title=\"📊 Cumulative Return Comparison\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Portfolio Value (Normalized)\",\n",
    "    template=\"plotly_white\",\n",
    "    width=950,\n",
    "    height=550,\n",
    "    legend=dict(x=0, y=1.1, orientation=\"h\"),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dfa26-23ec-43d4-9194-35bb00e6bca6",
   "metadata": {},
   "source": [
    "### 📰 Sentiment Scores via FinBERT\n",
    "\n",
    "Using FinBERT and NewsAPI, we gathered and scored headlines from the past 30 days for each NIFTY 30 stock. \n",
    "\n",
    "These scores will be used to adjust predicted returns before portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352e166-c820-4f8a-a503-37d79cb2f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT (PyTorch)\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Setup NewsAPI\n",
    "api_key = \"e1317700d21749b9973466e801a0cd58\"\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "# Time range\n",
    "today = datetime.today()\n",
    "last_month = (today - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "today_str = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# Your list of tickers\n",
    "tickers = [  # Include your full list\n",
    "    \"ADANIENT.NS\", \"ASIANPAINT.NS\", \"AXISBANK.NS\", \"BAJFINANCE.NS\", \"BHARTIARTL.NS\",\n",
    "    \"BPCL.NS\", \"COALINDIA.NS\", \"DIVISLAB.NS\", \"HCLTECH.NS\", \"HDFCBANK.NS\", \"HINDUNILVR.NS\",\n",
    "    \"ICICIBANK.NS\", \"INFY.NS\", \"ITC.NS\", \"KOTAKBANK.NS\", \"LT.NS\", \"MARUTI.NS\",\n",
    "    \"NESTLEIND.NS\", \"NTPC.NS\", \"ONGC.NS\", \"POWERGRID.NS\", \"RELIANCE.NS\", \"SBIN.NS\",\n",
    "    \"SUNPHARMA.NS\", \"TATAMOTORS.NS\", \"TCS.NS\", \"TECHM.NS\", \"TITAN.NS\", \"ULTRACEMCO.NS\",\n",
    "    \"WIPRO.NS\"\n",
    "]\n",
    "\n",
    "sentiment_scores = {}\n",
    "no_news_tickers = []\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    query = ticker.replace(\".NS\", \"\") + \" stock\"\n",
    "\n",
    "    try:\n",
    "        articles = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=last_month,\n",
    "            to=today_str,\n",
    "            language='en',\n",
    "            sort_by='relevancy',\n",
    "            page_size=30\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching news for {ticker}: {e}\")\n",
    "        sentiment_scores[ticker] = 0\n",
    "        continue\n",
    "\n",
    "    headlines = [a['title'] for a in articles['articles']]\n",
    "    if not headlines:\n",
    "        sentiment_scores[ticker] = 0\n",
    "        no_news_tickers.append(ticker)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        results = sentiment_model(headlines)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error scoring sentiment for {ticker}: {e}\")\n",
    "        sentiment_scores[ticker] = 0\n",
    "        continue\n",
    "\n",
    "    score = 0\n",
    "    for r in results:\n",
    "        label = r['label'].lower()\n",
    "        if label == 'positive':\n",
    "            score += 1\n",
    "        elif label == 'negative':\n",
    "            score -= 1\n",
    "\n",
    "    sentiment_scores[ticker] = round(score / len(results), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e4dbd-fadb-427d-9c8f-d3b3728040c8",
   "metadata": {},
   "source": [
    "### 📰 FinBERT Sentiment Scoring (Past 30 Days)\n",
    "\n",
    "This block collects the last 30 days of news headlines for each of the top 30 NIFTY stocks using NewsAPI, and scores them using the FinBERT transformer.\n",
    "\n",
    "- Each article's sentiment is classified as Positive, Negative, or Neutral.\n",
    "- The final sentiment score for a stock is calculated as:\n",
    "\n",
    "\\[\n",
    "\\text{Sentiment Score} = \\frac{\\#(\\text{Positive}) - \\#(\\text{Negative})}{\\text{Total Headlines}}\n",
    "\\]\n",
    "\n",
    "These scores will later be scaled and used to adjust predicted returns in the portfolio optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b657b51-6319-42d5-91ac-20fbc0cb5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load FinBERT model (PyTorch version) ===\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# === Setup NewsAPI ===\n",
    "api_key = \"e1317700d21749b9973466e801a0cd58\"\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "# === Date range: last 30 days ===\n",
    "today = datetime.today()\n",
    "last_month = (today - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "today_str = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# === Ticker list ===\n",
    "tickers = [\n",
    "    \"ADANIENT.NS\", \"ASIANPAINT.NS\", \"AXISBANK.NS\", \"BAJFINANCE.NS\", \"BHARTIARTL.NS\",\n",
    "    \"BPCL.NS\", \"COALINDIA.NS\", \"DIVISLAB.NS\", \"HCLTECH.NS\", \"HDFCBANK.NS\", \"HINDUNILVR.NS\",\n",
    "    \"ICICIBANK.NS\", \"INFY.NS\", \"ITC.NS\", \"KOTAKBANK.NS\", \"LT.NS\", \"MARUTI.NS\",\n",
    "    \"NESTLEIND.NS\", \"NTPC.NS\", \"ONGC.NS\", \"POWERGRID.NS\", \"RELIANCE.NS\", \"SBIN.NS\",\n",
    "    \"SUNPHARMA.NS\", \"TATAMOTORS.NS\", \"TCS.NS\", \"TECHM.NS\", \"TITAN.NS\", \"ULTRACEMCO.NS\",\n",
    "    \"WIPRO.NS\"\n",
    "]\n",
    "\n",
    "# === Storage ===\n",
    "sentiment_scores = {}\n",
    "no_news_tickers = []\n",
    "\n",
    "# === Sentiment Scoring Loop ===\n",
    "print(\"📥 Fetching and scoring sentiment for 30 tickers...\\n\")\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    query = ticker.replace(\".NS\", \"\") + \" stock\"\n",
    "\n",
    "    try:\n",
    "        articles = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=last_month,\n",
    "            to=today_str,\n",
    "            language='en',\n",
    "            sort_by='relevancy',\n",
    "            page_size=30\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API error for {ticker}: {e}\")\n",
    "        sentiment_scores[ticker] = 0\n",
    "        continue\n",
    "\n",
    "    headlines = [a['title'] for a in articles['articles'] if 'title' in a]\n",
    "    if not headlines:\n",
    "        sentiment_scores[ticker] = 0\n",
    "        no_news_tickers.append(ticker)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        results = sentiment_model(headlines)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Sentiment scoring error for {ticker}: {e}\")\n",
    "        sentiment_scores[ticker] = 0\n",
    "        continue\n",
    "\n",
    "    # Score aggregation\n",
    "    score = 0\n",
    "    for r in results:\n",
    "        label = r['label'].lower()\n",
    "        if label == 'positive':\n",
    "            score += 1\n",
    "        elif label == 'negative':\n",
    "            score -= 1\n",
    "\n",
    "    sentiment_scores[ticker] = round(score / len(results), 4)\n",
    "\n",
    "# === Output ===\n",
    "print(\"\\n📊 Final Sentiment Scores (last 30 days):\")\n",
    "for k, v in sentiment_scores.items():\n",
    "    print(f\"{k:15}: {v:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee12df9-6de1-40e5-a978-c6fb3ddf45e4",
   "metadata": {},
   "source": [
    "### 🧠 Sentiment-Adjusted Portfolio Performance\n",
    "\n",
    "We apply an overlay of FinBERT-based sentiment scores to predicted expected returns (`mu_pred`), scaled by an alpha parameter (0.5). A new portfolio is optimized using these sentiment-adjusted returns.\n",
    "\n",
    "- Comparison is run over the last 30 trading days to isolate recent sentiment impact.\n",
    "- Sharpe Ratios for both base and adjusted portfolios are shown below.\n",
    "\n",
    "The overlay demonstrates how even weak signals (e.g., from 30 days of headlines) can improve portfolio Sharpe with smart weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2811e-46f2-4c4b-87e7-efd7ac80525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  # Sensitivity to sentiment\n",
    "\n",
    "# --- Apply sentiment to predicted returns ---\n",
    "mu_sent = mu_pred.copy()\n",
    "for ticker in mu_sent.index:\n",
    "    mu_sent[ticker] *= (1 + alpha * sentiment_scores.get(ticker, 0))\n",
    "\n",
    "# --- Covariance matrix ---\n",
    "assets = list(set(mu_pred.index).intersection(returns.columns))\n",
    "S = returns[assets].cov()\n",
    "\n",
    "# --- Base Portfolio Optimization ---\n",
    "ef_base = EfficientFrontier(mu_pred.loc[assets], S)\n",
    "base_weights = ef_base.max_sharpe()\n",
    "base_cleaned = ef_base.clean_weights()\n",
    "w_base = pd.Series(base_cleaned)\n",
    "\n",
    "# --- Sentiment-adjusted Portfolio Optimization ---\n",
    "ef_sent = EfficientFrontier(mu_sent.loc[assets], S)\n",
    "sent_weights = ef_sent.max_sharpe()\n",
    "sent_cleaned = ef_sent.clean_weights()\n",
    "w_sent = pd.Series(sent_cleaned)\n",
    "\n",
    "# --- Simulate Portfolios ---\n",
    "returns_30d = returns[assets].iloc[-30:]  # last 30 days only\n",
    "\n",
    "base_returns = returns_30d @ w_base\n",
    "sent_returns = returns_30d @ w_sent\n",
    "base_cum = (1 + base_returns).cumprod()\n",
    "sent_cum = (1 + sent_returns).cumprod()\n",
    "\n",
    "# --- Plot Cumulative Returns ---\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Base Portfolio\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=base_cum.index,\n",
    "    y=base_cum.values,\n",
    "    mode='lines',\n",
    "    name='🔧 Base Portfolio',\n",
    "    line=dict(color='steelblue', width=3)\n",
    "))\n",
    "\n",
    "# Sentiment-Adjusted Portfolio\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sent_cum.index,\n",
    "    y=sent_cum.values,\n",
    "    mode='lines',\n",
    "    name='📰 Sentiment-Adjusted',\n",
    "    line=dict(color='crimson', dash='dash', width=3)\n",
    "))\n",
    "\n",
    "# Layout customization\n",
    "fig.update_layout(\n",
    "    title=\"📈 Cumulative Returns: Base vs Sentiment-Adjusted Portfolio (Last 30 Days)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Portfolio Value\",\n",
    "    template=\"plotly_white\",\n",
    "    width=950,\n",
    "    height=500,\n",
    "    legend=dict(x=0.01, y=1.1, orientation='h')\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# --- Sharpe Ratio Function ---\n",
    "def sharpe_ratio(returns, risk_free_rate=0.0, period=30):\n",
    "    recent = returns[-period:].dropna()\n",
    "    if recent.std() == 0: return np.nan\n",
    "    excess = recent - risk_free_rate\n",
    "    return (excess.mean() / excess.std()) * np.sqrt(252)\n",
    "\n",
    "# --- Sharpe Ratios ---\n",
    "sr_base = sharpe_ratio(base_returns)\n",
    "sr_sent = sharpe_ratio(sent_returns)\n",
    "\n",
    "print(\"\\n📊 Sharpe Ratios (Last 30 Days):\")\n",
    "print(f\"🔧 Base Portfolio         : {sr_base:.2f}\")\n",
    "print(f\"📰 Sentiment-Adjusted     : {sr_sent:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d0348-b89f-40eb-ad5a-3187d1d0d256",
   "metadata": {},
   "source": [
    "### 📈 Sentiment-Aware Daily Allocation\n",
    "\n",
    "We use FinBERT to evaluate today's market sentiment per stock. These scores are scaled and overlaid on predicted returns, modifying our expectations before running the mean-variance optimization.\n",
    "\n",
    "This updated portfolio is saved for backtesting and deployment.\n",
    "\n",
    "- Model: `FinBERT (yiyanghkust/finbert-tone)`\n",
    "- News Source: `NewsAPI` (last 24h)\n",
    "- Alpha: 0.5 (adjusts how much sentiment influences allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1b303-c22a-446e-9955-62e63d3cec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Tickers list and API key (define if not already)\n",
    "try:\n",
    "    tickers\n",
    "except NameError:\n",
    "    tickers = list(y.columns)  # fallback from your model if tickers not defined\n",
    "\n",
    "try:\n",
    "    api_key\n",
    "except NameError:\n",
    "    raise ValueError(\"Please set your NewsAPI key as `api_key`.\")\n",
    "\n",
    "\n",
    "# 1. LOAD FinBERT (PyTorch)\n",
    "\n",
    "print(\"🔍 Loading FinBERT sentiment model...\")\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# 2. FETCH TODAY'S SENTIMENT\n",
    "\n",
    "print(\"📰 Fetching today's sentiment...\")\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "sentiment_today = {}\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    query = ticker.replace(\".NS\", \"\") + \" stock\"\n",
    "    try:\n",
    "        articles = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=today_str,\n",
    "            to=today_str,\n",
    "            language='en',\n",
    "            page_size=10\n",
    "        )\n",
    "        headlines = [a['title'] for a in articles['articles']]\n",
    "        if not headlines:\n",
    "            sentiment_today[ticker] = 0\n",
    "            continue\n",
    "\n",
    "        results = sentiment_model(headlines)\n",
    "        score = sum(\n",
    "            1 if r['label'].lower() == 'positive' else -1 if r['label'].lower() == 'negative' else 0\n",
    "            for r in results\n",
    "        )\n",
    "        sentiment_today[ticker] = round(score / len(results), 4)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {ticker}: {e}\")\n",
    "        sentiment_today[ticker] = 0\n",
    "\n",
    "\n",
    "# 3. Compute mu_pred from y_pred\n",
    "\n",
    "print(\"📈 Adjusting predicted returns using sentiment...\")\n",
    "mu_pred = pd.Series(y_pred.mean(axis=0), index=tickers)\n",
    "\n",
    "# Adjust with sentiment overlay\n",
    "mu_sent = mu_pred * (1 + alpha * pd.Series(sentiment_today))\n",
    "\n",
    "\n",
    "# 4. Load Covariance Matrix\n",
    "\n",
    "returns = pd.read_csv(\"daily_returns.csv\", index_col=0, parse_dates=True).dropna(axis=1)\n",
    "cov_matrix = returns[tickers].cov()\n",
    "\n",
    "\n",
    "# 5. Optimize Portfolio (Max Sharpe)\n",
    "\n",
    "print(\"🔧 Optimizing portfolio with sentiment-adjusted returns...\")\n",
    "ef = EfficientFrontier(mu_sent, cov_matrix)\n",
    "weights_sent = ef.max_sharpe()\n",
    "cleaned_weights = ef.clean_weights()\n",
    "\n",
    "\n",
    "# 6. Save Final Weights\n",
    "\n",
    "weights_df = pd.DataFrame.from_dict(cleaned_weights, orient='index', columns=[\"Weight\"])\n",
    "weights_df.index.name = \"Ticker\"\n",
    "weights_df.to_csv(f\"weights_{today_str}.csv\")\n",
    "\n",
    "print(f\"✅ Saved sentiment-adjusted weights to: weights_{today_str}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5ecd3-d0f1-437c-b229-6866d701e8c6",
   "metadata": {},
   "source": [
    "### 💰 Capital Allocation (₹1,00,000)\n",
    "\n",
    "Using the optimized weights from the sentiment-adjusted portfolio, we allocated capital proportionally across selected stocks. Assets with 0% weight were excluded for clarity. This allocation is saved daily for tracking and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7604bcc-7a1a-4713-96c9-ab2a49b74999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💰 Allocate ₹1,00,000 Based on Weights\n",
    "portfolio_value = 100_000  # ₹1,00,000\n",
    "\n",
    "# Filter and sort non-zero weights\n",
    "allocation_df = weights_df[weights_df[\"Weight\"] > 0].copy()\n",
    "allocation_df[\"Allocated Amount (Rs.)\"] = allocation_df[\"Weight\"] * portfolio_value\n",
    "allocation_df = allocation_df.sort_values(\"Allocated Amount (Rs.)\", ascending=False)\n",
    "\n",
    "# Save allocation\n",
    "allocation_file = f\"allocation_{today_str}.csv\"\n",
    "allocation_df.to_csv(allocation_file)\n",
    "\n",
    "# Print and display\n",
    "print(f\"📦 Saved allocation to: {allocation_file}\")\n",
    "print(\"\\n📊 Allocation for ₹1,00,000 Portfolio:\")\n",
    "display(allocation_df.style.format({\n",
    "    \"Weight\": \"{:.2%}\",\n",
    "    \"Allocated Amount (Rs.)\": \"₹{:,.0f}\"\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
